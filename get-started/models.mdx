---
title: "Models, Endpoints, and Pricing"
---

<Card title="Start Free with 10M Tokens" icon="sparkles">
  <Info>
    sEvery new account includes 10 million free tokens.
  </Info>
</Card>

## Production Models

| Model             | Input Price (1M Tokens) | Output Price (1M Tokens) | Supported Endpoints                                                                     | Context Window | Features                            | Supported Formats |
| ----------------- | ----------------------- | :----------------------- | --------------------------------------------------------------------------------------- | -------------- | ----------------------------------- | ----------------- |
| **Mercury**       | \$0.25                  | \$1.00                   | `v1/chat/completions`                                                                   | 128K           | `Tool Calling` `Structured Outputs` | `Text`            |
| **Mercury Coder** | \$0.25                  | \$1.00                   | `v1/chat/completions` `v1/fim/completions` `v1/apply/completions` `v1/edit/completions` | 128K           | `Tool Calling` `Structured Outputs` | `Text`            |

## API Specs

<Tabs>
  <Tab title="Mercury">
    Generalist model

    <ParamField path="max_tokens" default="8192" type="number">
      Maximum number of tokens to generate. Range: 1–16384
    </ParamField>
    <ParamField path="frequency_penalty" default="0.0" type="number">
      Penalizes new tokens based on their frequency in the generated text so far. Range: -2.0-2.0
    </ParamField>
    <ParamField path="presence_penalty" default="1.5" type="number">
      Penalizes new tokens based on whether they appear in the generated text so far. Range: -2.0-2.0
    </ParamField>
    <ParamField path="temperature" default="0.0" type="number">
      Controls randomness. Range: 0.0-0.7
    </ParamField>
    <ParamField path="top_p" default="1.0" type="number">
      Controls the cumulative probability of the top tokens to consider. Range: 0.0–1.0
    </ParamField>
    <ParamField path="top_k" default="null" type="number">
      Controls the number of top tokens to consider. Range: 1–1000
    </ParamField>
    <ParamField path="stop" default="null" type="string[]">
      Up to 4 sequences where the model will stop generating further tokens. The returned text will not contain these sequences.
    </ParamField>
    <ParamField path="stream" default="false" type="boolean">
      Whether to stream the response.
    </ParamField>
    <ParamField path="stream_options" default="null" type="object">
      Include include_usage=true to get usage information.
    </ParamField>
    <ParamField path="diffusing" default="false" type="boolean">
      Streaming should be set to true for diffusing effect.
    </ParamField>
    <ParamField path="tools" default="null" type="object[]">
      A list of tools the model may call.
    </ParamField>
    ## Example Usage

    ### Chat Completions

    `v1/chat/completions`

    <CodeGroup>

    ```bash cURL
    curl https://api.inceptionlabs.ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer INCEPTION_API_KEY" \
      -d '{
        "model": "mercury",
        "messages": [
          {"role": "user", "content": "Hello! What is a diffusion model?"}
        ],
        "max_tokens": 1000
      }'
    ```

    
    ```javascript JavaScript
    // Using fetch API
    const response = await fetch('https://api.inceptionlabs.ai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
      },
      body: JSON.stringify({
        model: "mercury",
        messages: [
          { role: 'user', content: 'Hello! What is a diffusion model?' }
        ],
        max_tokens: 1000
      })
    });
    
    const data = await response.json();
    console.log(data.choices[0].message.content);
    ```

    
    ```python Python
    import requests
    
    response = requests.post('https://api.inceptionlabs.ai/v1/chat/completions', headers={
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
    }, json={
        "model": "mercury",
        "messages": [
            {"role": "user", "content": "Hello! What is a diffusion model?"}
        ],
        "max_tokens": 1000
    })
    
    print(response.json()['choices'][0]['message']['content'])
    ```

    </CodeGroup>
  </Tab>
  <Tab title="Mercury Coder">
    Coding specialist model

    <ParamField path="max_tokens" default="8192" type="number">
      Maximum number of tokens to generate. Range: 1–16384
    </ParamField>
    <ParamField path="frequency_penalty" default="0.0" type="number">
      Penalizes new tokens based on their frequency in the generated text so far. Range: -2.0-2.0
    </ParamField>
    <ParamField path="presence_penalty" default="1.5" type="number">
      Penalizes new tokens based on whether they appear in the generated text so far. Range: -2.0-2.0
    </ParamField>
    <ParamField path="temperature" default="0.0" type="number">
      Controls randomness. Range: 0.0-0.7
    </ParamField>
    <ParamField path="top_p" default="1.0" type="number">
      Controls the cumulative probability of the top tokens to consider. Range: 0.0–1.0
    </ParamField>
    <ParamField path="top_k" default="null" type="number">
      Controls the number of top tokens to consider. Range: 1–1000
    </ParamField>
    <ParamField path="stop" default="null" type="string[]">
      Up to 4 sequences where the model will stop generating further tokens. The returned text will not contain these sequences.
    </ParamField>
    <ParamField path="stream" default="false" type="boolean">
      Whether to stream the response.
    </ParamField>
    <ParamField path="stream_options" default="null" type="object">
      Include include_usage=true to get usage information.
    </ParamField>
    <ParamField path="diffusing" default="false" type="boolean">
      Streaming should be set to true for diffusing effect.
    </ParamField>
    <ParamField path="tools" default="null" type="object[]">
      A list of tools the model may call.
    </ParamField>
    ## Example Usage

    ### Chat Completions

    `v1/chat/completions`

    <CodeGroup>

    ```bash cURL
    curl https://api.inceptionlabs.ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer INCEPTION_API_KEY" \
      -d '{
        "model": "mercury-coder",
        "messages": [
          {"role": "user", "content": "Hello! What is 1+1?"}
        ],
        "max_tokens": 1000
      }'
    ```

    
    ```javascript JavaScript
    // Using fetch API
    const response = await fetch('https://api.inceptionlabs.ai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
      },
      body: JSON.stringify({
        model: "mercury-coder",
        messages: [
          { role: 'user', content: 'Hello! What is 1+1?' }
        ],
        max_tokens: 1000
      })
    });
    
    const data = await response.json();
    console.log(data.choices[0].message.content);
    ```

    
    ```python Python
    import requests
    
    response = requests.post('https://api.inceptionlabs.ai/v1/chat/completions', headers={
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
    }, json={
        "model": "mercury-coder",
        "messages": [
            {"role": "user", "content": "Hello! What is a diffusion model?"}
        ],
        "max_tokens": 1000
    })
    
    print(response.json()['choices'][0]['message']['content'])
    ```

    </CodeGroup>

    ### Fill-in-the-Middle

    `v1/fim/completions`

    <CodeGroup>

    ```bash cURL
    curl https://api.inceptionlabs.ai/v1/fim/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer INCEPTION_API_KEY" \
      -d '{
        "model": "mercury-coder",
        "prompt": "def fibonacci(",
        "suffix": "return a + b",
        "max_tokens": 1000
      }'
    ```

    
    ```javascript JavaScript
    // Using fetch API
    const response = await fetch('https://api.inceptionlabs.ai/v1/fim/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
      },
      body: JSON.stringify({
        model: "mercury-coder",
        prompt: "def fibonacci(",
        suffix: "return a + b",
        max_tokens: 1000
      })
    });
    
    const data = await response.json();
    console.log(data.choices[0].text);
    ```

    
    ```python Python
    import requests
    
    response = requests.post('https://api.inceptionlabs.ai/v1/fim/completions', headers={
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
    }, json={
        "model": "mercury-coder",
        "prompt": "def fibonacci(",
        "suffix": "return a + b",
        "max_tokens": 1000
    })
    
    print(response.json()['choices'][0]['text'])
    ```

    </CodeGroup>

    ### Apply-Edit

    `v1/apply/completions`

    <CodeGroup>

    ```bash cURL
    curl https://api.inceptionlabs.ai/v1/apply/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer INCEPTION_API_KEY" \
      --data-binary @- <<'JSON'
    {
      "model": "mercury-coder",
      "messages": [
        {"role": "user", "content": "<|original_code|>\nclass Calculator:\n    \"\"\"A simple calculator class.\"\"\"\n    def __init__(self):\n        self.history = []\n\n    def add(self, a, b):\n        \"\"\"Adds two numbers.\"\"\"\n        result = a + b\n        return result\n<|/original_code|>\n\n<|update_snippet|>\n// ... existing code ...\ndef multiply(self, a, b):\n    \"\"\"Multiplies two numbers.\"\"\"\n    result = a * b\n    return result\n// ... existing code ...\n<|/update_snippet|>"}
      ],
      "max_tokens": 1000
    }
    JSON
    ```

    
    ```javascript JavaScript
    // Using fetch API
    const response = await fetch('https://api.inceptionlabs.ai/v1/apply/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
      },
      body: JSON.stringify({
        model: "mercury-coder",
        messages: [
          { role: 'user', content: '<|original_code|>\nclass Calculator:\n    \"\"\"A simple calculator class.\"\"\"\n    def __init__(self):\n        self.history = []\n\n    def add(self, a, b):\n        \"\"\"Adds two numbers.\"\"\"\n        result = a + b\n        return result\n<|/original_code|>\n\n<|update_snippet|>\n// ... existing code ...\ndef multiply(self, a, b):\n    \"\"\"Multiplies two numbers.\"\"\"\n    result = a * b\n    return result\n// ... existing code ...\n<|/update_snippet|>' }
        ],
        max_tokens: 1000
      })
    });
    
    const data = await response.json();
    console.log(data.choices[0].message.content);
    ```

    
    ```python Python
    import requests
    
    response = requests.post('https://api.inceptionlabs.ai/v1/apply/completions', headers={
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
    }, json={
        "model": "mercury-coder",
        "messages": [
            {"role": "user", "content": "<|original_code|>\nclass Calculator:\n    \"\"\"A simple calculator class.\"\"\"\n    def __init__(self):\n        self.history = []\n\n    def add(self, a, b):\n        \"\"\"Adds two numbers.\"\"\"\n        result = a + b\n        return result\n<|/original_code|>\n\n<|update_snippet|>\n// ... existing code ...\ndef multiply(self, a, b):\n    \"\"\"Multiplies two numbers.\"\"\"\n    result = a * b\n    return result\n// ... existing code ...\n<|/update_snippet|>"}
        ],
        "max_tokens": 1000
    })
    
    print(response.json()['choices'][0]['message']['content'])
    ```

    </CodeGroup>

    ### Next-Edit

    `v1/edit/completions`

    <CodeGroup>

    ```bash cURL
    curl https://api.inceptionlabs.ai/v1/edit/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer INCEPTION_API_KEY" \
      --data-binary @- <<'JSON'
    {
      "model": "mercury-coder",
      "messages": [
        {"role": "user", "content": "<|recently_viewed_code_snippets|>\n\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: solver.py\n'''\nfunction: flagAllNeighbors\n----------\nThis function marks each of the covered neighbors of the cell at the given row\n<|code_to_edit|>\nand col as flagged.\n'''\ndef flagAllNeighbors(board<|cursor|>, row, col): \n for r, c in b.getNeighbors(row, col):\n if b.isValid(r, c):\n b.flag(r, c)\n\n<|/code_to_edit|>\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/test/testing/solver.py\n+++ /c:/Users/test/testing/solver.py\n@@ -6,1 +6,1 @@\n-def flagAllNeighbors(b, row, col): \n+def flagAllNeighbors(board, row, col): \n\n<|/edit_diff_history|>"}
      ],
      "max_tokens": 1000
    }
    JSON
    ```

    
    ```javascript JavaScript
    // Using fetch API
    const response = await fetch('https://api.inceptionlabs.ai/v1/edit/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
      },
      body: JSON.stringify({
        model: "mercury-coder",
        messages: [
          { role: 'user', content: '<|recently_viewed_code_snippets|>\n\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: solver.py\n\'\'\'\nfunction: flagAllNeighbors\n----------\nThis function marks each of the covered neighbors of the cell at the given row\n<|code_to_edit|>\nand col as flagged.\n\'\'\'\ndef flagAllNeighbors(board<|cursor|>, row, col): \n for r, c in b.getNeighbors(row, col):\n if b.isValid(r, c):\n b.flag(r, c)\n\n<|/code_to_edit|>\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/test/testing/solver.py\n+++ /c:/Users/test/testing/solver.py\n@@ -6,1 +6,1 @@\n-def flagAllNeighbors(b, row, col): \n+def flagAllNeighbors(board, row, col): \n\n<|/edit_diff_history|>' }
        ],
        max_tokens: 1000
      })
    });
    
    const data = await response.json();
    console.log(data.choices[0].text);
    ```

    
    ```python Python
    import requests
    
    response = requests.post('https://api.inceptionlabs.ai/v1/edit/completions', headers={
        'Content-Type': 'application/json',
        'Authorization': 'Bearer INCEPTION_API_KEY'
    }, json={
        "model": "mercury-coder",
        "messages": [
            {"role": "user", "content": "<|recently_viewed_code_snippets|>\n\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: solver.py\n'''\nfunction: flagAllNeighbors\n----------\nThis function marks each of the covered neighbors of the cell at the given row\n<|code_to_edit|>\nand col as flagged.\n'''\ndef flagAllNeighbors(board<|cursor|>, row, col): \n for r, c in b.getNeighbors(row, col):\n if b.isValid(r, c):\n b.flag(r, c)\n\n<|/code_to_edit|>\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/test/testing/solver.py\n+++ /c:/Users/test/testing/solver.py\n@@ -6,1 +6,1 @@\n-def flagAllNeighbors(b, row, col): \n+def flagAllNeighbors(board, row, col): \n\n<|/edit_diff_history|>"}
        ],
        "max_tokens": 1000
    })
    
    print(response.json()['choices'][0]['text'])
    ```

    </CodeGroup>
  </Tab>
</Tabs>